\section{Performance Evaluation}
\label{sec:eval}

A crucial part of research is reporting results and comparing achievements with other state-of-the-art work. Unfortunately there is no standard way of fulfilling that task, this leads to a hard 
We would like to assist with the following considerations.


\subsection{Hardware-Independent}
A brief description of the \emph{network topology} is most welcome, we believe different topologies will have a deep impact on the overall performance. Furthermore, sharing this designs can inspire fellow scientists to create new structures that, might  a new point-of-view to bloom, generating a positive feedback loop where everybody wins.

Most classification papers report a percentage of accuracy that gives the reader a measure of the correct classifications~[\cite{dietterich1998approximate}]. Some times it might be desirable, for a better understanding of the paper, that a distinction between ambiguous, outliers and incorrect classes is made~[\cite{liu2002performance}]. A very useful piece of information is clear citation of the base-line source, which is almost always there but lost in a sea of references.

%Should we report also incorrect or ambiguous? Could some ``correct'' be masking ambiguous? Were the ambiguous due to noise? Was the noise added on purpose? 

As we are proposing spike based data-sets, it's desirable that the users specify if there was any preprocessing applied before actually feeding the spike trains into their networks~[\cite{best-practice-nn-img}]. For example, if we want to use a particular set to test noisy inputs, it would be extremely useful to have a notion of the type of noise added.

%Traditionally, neural network training has been done using rate-based encoding. As new theories emerge, a

An important distinction to make is the nature of the training procedure. One example is the way data was exposed to the network (e.g. How many times each image was presented? How much time was a single example shown?. [\cite{unsup_leraning_diehl}]) Also, details on the particulars of the implementation of the learning rule used (e.g. Delta, STDP, BCM, etc.) is highly desirable. 

One the biggest distinctions on learning procedures is whether they were done using some \emph{supervision} or not; making this distinction clear is vastly appreciated. On supervised learning, the label of the data influences to establish categories and connection weights. Unsupervised learning has fewer constraints when it comes to class creation but might be tougher to get right. 

A number of different classes are expected, this quantity might give an insight onto the network topology and dynamics. A description of the methods used to generate and populate the classes is very helpful for the reader. (e.g. Did we use a statistical measure? Was it a combination of NN with some other algorithms?)

\begin{table*}
  \caption{Hardware independent comparison}
  \begin{center}
    \bgroup
    \def\arraystretch{1.4}
    \begin{tabular}{ l | c c c c c c }
      $ $ &
      \begin{minipage}{1.9cm}Topology \end{minipage} & 
      \begin{minipage}{1.9cm}Accuracy \end{minipage} & 
      \begin{minipage}{1.9cm}Preprocessing \end{minipage} &
      \begin{minipage}{1.9cm}Training \end{minipage} & 
      \begin{minipage}{1.9cm}Supervised \end{minipage} &
      \begin{minipage}{1.9cm}Extra classifier \end{minipage} \\
      \hline
%contents
      \begin{minipage}{2cm} Paper 1 [ref] \end{minipage}  & & & & & & \\
      \begin{minipage}{2cm} Paper 2 [ref]\end{minipage}  & & & & & & \\
      \begin{minipage}{2cm} Paper 3 [ref]\end{minipage}  & & & & & & \\
      \begin{minipage}{2cm} Paper 4 [ref]\end{minipage}  & & & & & & 
    \end{tabular}
    \egroup
  \end{center}
  \label{tb:software_comparison}
\end{table*}


\subsection{Hardware-Specific}
Specifying hardware is of utmost importance when comparing computing times and power consumption. Analog or digital or a hybrid system.

Different platforms have special benefits, purely hardware solutions have low power consumption but lack programmability.

New theories, such as Polychronization, suggest that axonal delays are an integral part of the brain's computing mechanisms~[\cite{Izhikevich2005}]. 

Power consumption is a key issue for mobile applications and robotics. A way to measure is to state the number of \emph{Synaptic operations per Watt} that the hardware is capable of.

An important factor to measure performance is the number of \emph{Synaptic events per second}; i.e. rough throughput

A piece of hardware that is difficult to use/program is of little use, thus \emph{front-end support} is

Neural activity highly depends on synapses, specifying what model was used and its precision will impact on the performance.

\begin{table*}
  \caption{Hardware dependant comparison}
  \begin{center}
      \bgroup
      \def\arraystretch{1.4}
    \begin{tabular}{l | c c c c c c c c c}
      $ $ & 
       \begin{minipage}{1.2cm}\centering Hardware approach \end{minipage} & 
       \begin{minipage}{1.3cm}\centering Simulation type \end{minipage} & 
       \begin{minipage}{1.7cm}\centering Programmable \end{minipage} & 
       \begin{minipage}{1cm}\centering Axonal delays \end{minipage} & 
       \begin{minipage}{1cm}\centering Synaptic model \end{minipage} & 
       \begin{minipage}{1.2cm}\centering Synaptic precision \end{minipage} & 
       \begin{minipage}{1.2cm}\centering Synaptic events per sec \end{minipage} & 
       \begin{minipage}{1.4cm}\centering Synaptic ops per Watt \end{minipage} & 
       \begin{minipage}{1.7cm}\centering Programming front-end \end{minipage}  \\
       \hline
       % contents!
       \begin{minipage}{1.8cm}\centering Paper 1 [ref] \end{minipage} & & & & & & & & & \\
       \begin{minipage}{1.8cm}\centering Paper 2 [ref]\end{minipage} & & & & & & & & & \\
       \begin{minipage}{1.8cm}\centering Paper 3 [ref]\end{minipage} & & & & & & & & & \\
       \begin{minipage}{1.8cm}\centering Paper 4 [ref]\end{minipage} & & & & & & & & & 
    \end{tabular}
    \egroup
  \end{center}
  \label{tb:hardware_comparison}
\end{table*}
    
%table summary?
