\section{Guiding Principles}
\label{sec:guide}
The NE (Neuromorphic Engineering) database we propose here is a developing and evolving dataset consisting various spike-based representations of images and videos.
The spikes are either generated from spike encoding methods which covert images or frames of videos into spiking trains, or recorded from DVS silicon retinas.
The spikes trains are in the format of AER data, which could be easily used in both event-driven computer simulations and neuromorphic systems.
With the NE dataset we hope:
\begin{itemize}
	\item \textit{to promote meaningful comparison among algorithms in the field of spiking neural computation.}
	The NE dataset provides unified format of AER data to meet the demand of spike-based visual stimulus.
	It also encourages researchers to publish and contribute their data to build up the NE dataset.
	The training and testing sets have to be disjoint and also of similar quality and quantity.
	\item \textit{to allow comparison with conventional image recognition methods.}
	It asks the dataset to support this comparison with spiking versions of existing vision datasets.
	As the first publish of the dataset, NE15 is composed of four subsets of spiking versions of MNIST, thus to compare the recognition performance with conventional methods. 
	\item \textit{to provide an assessment of the state of the art in spike-based visual recognition on neuromorphic hardware.}
	In order to reveal the advantages of neuromorphic engineering, not only a spike based dataset but also an appropriate evaluation is needed.
	In accordance with the idea of an evolving dataset, the evaluation methodology develops accordingly as a constantly perfected process.
	\item \textit{to help researchers identify future directions and advance the field.}
	The development of the dataset and its evaluation will introduce new challenges to the neuromorphic engineering community.
	However, an easily solved problem turns out to be a tuning competition, while a far beyond difficult problem is not able to bring meaningful assessment.
	So balanced data should be added continuously to promote the future research.  
\end{itemize}
%\subsection{Dataset and Testing Protocols Referring the Goals}

%The FERET database was established to support both
%algorithm development and evaluation. Two guiding prin-
%ciples were followed. First, the evaluation of algorithms
%requires a common database of images for both develop-
%ment and testing. In the FERET evaluation, the images in
%the test are from both the development
%and sequestered
%portions of the FERET database. Second, the variety and
%difficulty of the problems defined by the images in the data-
%base must increase incrementally.
%The need to test algorithms against a database is obvious,
%but the development
%function of the database is equally
%important (if less obvious). For the evaluation procedure
%to produce meaningful results, the images in the develop-
%mental portion of the database must resemble those on
%which algorithms are to be tested. The development and
%testing data sets must be similar in both quality and quantity.
%For example, if the test will consist of a gallery of 1000
%individuals,
%it is not appropriate
%for the development
%database to consist of 50 individuals. The algorithms tested
%will be only as good as the database from which they are
%developed. The FERET evaluation procedure followed this
%principle by partitioning the FERET database into the devel-
%opmental and sequestered portions, where the developmen-
%tal portion was representative
%of the sequestered portion
%(details are provided in Section 4.2).
%The other principle is that the problem defined by the
%images in the database must mesh with the current level
%of algorithm development, and the difficulty of the database
%must grow as the sophistication of the algorithms increases.
%As explained in Section 2, if the database defines a problem
%that is too easy, testing the algorithm becomes a mere tuning
%exercise. At the other extreme, if the problem is too far
%beyond the state of the art, the test will not produce any
%meaningful results. To prevent the FERET database from
%becoming stale, we continuously expanded and adjusted the
%database to the state of the art in face recognition.

%\subsection{The Proposal: Advantages (Move to the conclusion)}
%In this paper we present a comprehensive method of eval-
%uating face-recognition
%algorithms, developed as part of the
%Face Recognition Technology (FERET) program [ 10,111.
%The FERET evaluation methodology consists of an inte-
%grated data collection effort and testing program. These
%two parts are integrated through the FERET database of
%facial images; the database is divided into a development
%portion, which is provided to researchers, and a sequestered
%portion for testing. The partition of the database enables
%algorithms to be trained and tested on different, but related,
%sets of images. The FERET evaluation procedure is a set of
%standard testing protocols: the FERET tests are indepen-
%dently administered
%and each test is completed within
%three days. The use of a standard testing protocol allows
%for the direct comparison of algorithms developed by dif-
%ferent groups, as well as for measuring improvements made
%by any single group over time.

