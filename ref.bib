@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {0018-9219},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	keywords = {2D shape variability, backpropagation, back-propagation, character recognition, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition, document recognition systems, Feature extraction, field extraction, gradient-based learning, gradient based learning technique, graph transformer networks, GTN, handwritten character recognition, handwritten digit recognition task, hidden Markov models, high-dimensional patterns, language modeling, machine learning, Multi-layer neural network, multilayer neural networks, multilayer perceptrons, multimodule systems, Neural networks, optical character recognition, Optical character recognition software, Optical computing, Pattern Recognition, performance measure minimization, Principal component analysis, segmentation recognition},
	pages = {2278--2324},
	file = {IEEE Xplore Abstract Record:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/RDDSDJHU/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/G3CT5B75/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf}
}
@inproceedings{deng_imagenet:_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, 2009. {CVPR} 2009},
	author = {Deng, Jia and Dong, Wei and Socher, R. and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	keywords = {computer vision, Explosions, Image databases, ImageNet database, image resolution, image retrieval, Information retrieval, Internet, large-scale hierarchical image database, large-scale ontology, Large-scale systems, multimedia computing, multimedia data, Multimedia databases, Ontologies, ontologies (artificial intelligence), Robustness, Spine, subtree, trees (mathematics), very large databases, visual databases, wordNet structure},
	pages = {248--255},
	file = {IEEE Xplore Abstract Record:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/TFFAKIMS/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/QMNG4F75/Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf:application/pdf}
}

@inproceedings{liu_recognizing_2009,
	title = {Recognizing realistic actions from videos \#x201C;in the wild \#x201D;},
	doi = {10.1109/CVPR.2009.5206744},
	abstract = {In this paper, we present a systematic framework for recognizing realistic actions from videos “in the wild”. Such unconstrained videos are abundant in personal collections as well as on the Web. Recognizing action from such videos has not been addressed extensively, primarily due to the tremendous variations that result from camera motion, background clutter, changes in object appearance, and scale, etc. The main challenge is how to extract reliable and informative features from the unconstrained videos. We extract both motion and static features from the videos. Since the raw features of both types are dense yet noisy, we propose strategies to prune these features. We use motion statistics to acquire stable motion features and clean static features. Furthermore, PageRank is used to mine the most informative static features. In order to further construct compact yet discriminative visual vocabularies, a divisive information-theoretic algorithm is employed to group semantically related features. Finally, AdaBoost is chosen to integrate all the heterogeneous yet complementary features for recognition. We have tested the framework on the KTH dataset and our own dataset consisting of 11 categories of actions collected from YouTube and personal videos, and have obtained impressive results for action recognition and action localization.},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, 2009. {CVPR} 2009},
	author = {Liu, Jingen and Luo, Jiebo and Shah, M.},
	month = jun,
	year = {2009},
	keywords = {action localization, AdaBoost, cameras, computer vision, Feature extraction, Humans, image motion analysis, information-theoretic algorithm, informative static features, KTH dataset, motion features, Motion pictures, motion statistics, PageRank, personal videos, realistic action recognition, Shape, spatiotemporal phenomena, unconstrained videos, Videos, video signal processing, visual vocabularies, Vocabulary, YouTube},
	pages = {1996--2003},
	file = {IEEE Xplore Abstract Record:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/DVK7SNJD/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/7VFBCHUQ/Liu et al. - 2009 - Recognizing realistic actions from videos #x201C\;i.pdf:application/pdf}
}

@article{lin_microsoft_2014,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2015-06-09},
	journal = {arXiv:1405.0312 [cs]},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = may,
	year = {2014},
	note = {arXiv: 1405.0312},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1405.0312 PDF:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/EM38WHHN/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf:application/pdf;arXiv.org Snapshot:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/84JJU8BA/1405.html:text/html}
}

@article{la2008response,
  title={The response of cortical neurons to in vivo-like input current: theory and experiment},
  author={La Camera, Giancarlo and Giugliano, Michele and Senn, Walter and Fusi, Stefano},
  journal={Biological cybernetics},
  volume={99},
  number={4-5},
  pages={279--301},
  year={2008},
  publisher={Springer}
}

@article{siegert1951first,
  title={On the first passage time probability problem},
  author={Siegert, Arnold JF},
  journal={Physical Review},
  volume={81},
  number={4},
  pages={617},
  year={1951},
  publisher={APS}
}

@article{burkitt2006review,
  title={A review of the integrate-and-fire neuron model: I. Homogeneous synaptic input},
  author={Burkitt, Anthony N},
  journal={Biological cybernetics},
  volume={95},
  number={1},
  pages={1--19},
  year={2006},
  publisher={Springer}
}


@inproceedings{delbruck2008frame,
  title={Frame-free dynamic digital vision},
  author={Delbruck, Tobi},
  booktitle={Proceedings of Intl. Symp. on Secure-Life Electronics, Advanced Electronics for Quality Life and Society},
  pages={21--26},
  year={2008}
}

@article{serrano-gotarredona_128_2013,
	title = {A 128 128 1.5\% {Contrast} {Sensitivity} 0.9\% {FPN} 3 \#x00B5;s {Latency} 4 {mW} {Asynchronous} {Frame}-{Free} {Dynamic} {Vision} {Sensor} {Using} {Transimpedance} {Preamplifiers}},
	volume = {48},
	issn = {0018-9200},
	doi = {10.1109/JSSC.2012.2230553},
	abstract = {Dynamic Vision Sensors (DVS) have recently appeared as a new paradigm for vision sensing and processing. They feature unique characteristics such as contrast coding under wide illumination variation, micro-second latency response to fast stimuli, and low output data rates (which greatly improves the efficiency of post-processing stages). They can track extremely fast objects (e.g., time resolution is better than 100 kFrames/s video) without special lighting conditions. Their availability has triggered a new range of vision applications in the fields of surveillance, motion analyses, robotics, and microscopic dynamic observations. One key DVS feature is contrast sensitivity, which has so far been reported to be in the 10-15\% range. In this paper, a novel pixel photo sensing and transimpedance pre-amplification stage makes it possible to improve by one order of magnitude contrast sensitivity (down to 1.5\%) and power (down to 4 mW), reduce the best reported FPN (Fixed Pattern Noise) by a factor of 2 (down to 0.9\%), while maintaining the shortest reported latency (3 μs) and good Dynamic Range (120 dB), and further reducing overall area (down to 30 × 31 μm per pixel). The only penalty is the limitation of intrascene Dynamic Range to 3 decades. A 128 × 128 DVS test prototype has been fabricated in standard 0.35 μm CMOS and extensive experimental characterization results are provided.},
	number = {3},
	journal = {IEEE Journal of Solid-State Circuits},
	author = {Serrano-Gotarredona, T. and Linares-Barranco, B.},
	month = mar,
	year = {2013},
	keywords = {Address-event-representation, Arrays, asynchronous frame-free DVS, asynchronous frame-free dynamic vision sensor, CMOS image sensors, CMOS process, contrast coding, contrast sensitivity, Dynamic range, dynamic vision sensor, event-based vision, fixed pattern noise, FPN, frame-free vision sensor, high-speed vision, illumination variation, Lighting, lighting conditions, magnitude contrast sensitivity, microscopic dynamic observations, microsecond latency response, motion analyses, operational amplifiers, pixel photosensing, power 4 mW, preamplifiers, robotics, Robot sensing systems, Sensitivity, size 0.35 mum, temporal contrast retina, time 3 mus, transimpedance preamplification stage, transimpedance preamplifiers, Transistors, vision sensor, Voltage control},
	pages = {827--838},
	file = {IEEE Xplore Abstract Record:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/28N6SMEN/articleDetails.html:text/html;IEEE Xplore Full Text PDF:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/5RB3NFAK/Serrano-Gotarredona and Linares-Barranco - 2013 - A 128 128 1.5% Contrast Sensitivity 0.9% FPN 3 #x0.pdf:application/pdf}
}
@article{davison2008pynn,
  title={PyNN: a common interface for neuronal network simulators},
  author={Davison, Andrew P and Br{\"u}derle, Daniel and Eppler, Jochen and Kremkow, Jens and Muller, Eilif and Pecevski, Dejan and Perrinet, Laurent and Yger, Pierre},
  journal={Frontiers in neuroinformatics},
  volume={2},
  year={2008},
  publisher={Frontiers Research Foundation}
}
@book{squire1998findings,
  title={Findings and current opinion in cognitive neuroscience},
  author={Squire, Larry R and Kosslyn, Stephen Michael},
  year={1998},
  publisher={MIT Press}
}
@article{gewaltig2007nest,
  title={NEST (neural simulation tool)},
  author={Gewaltig, Marc-Oliver and Diesmann, Markus},
  journal={Scholarpedia},
  volume={2},
  number={4},
  pages={1430},
  year={2007}
}
