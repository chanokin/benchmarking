\section{Performance Evaluation}
\label{sec:eval}

When we come to the stage where we have to report results of our research, we often face questions like, \emph{How much should I include in my report? How do I compare my work to others?} These are extremely important questions, and we would like to assist with the following considerations.

\subsection{Hardware-Independent}
A brief description of the \emph{network topology} is most welcome, we believe different topologies will have a deep impact on the overall performance. Furthermore, sharing this designs can inspire fellow scientists to create new structures that, might  a new point-of-view to bloom, generating a positive feedback loop where everybody wins.

Most classification papers report a percentage of accuracy that gives the reader a measure of the correct classifications~[\cite{dietterich1998approximate}]. Some times it might be desirable, for a better understanding of the paper, that a distinction between ambiguous, outliers and incorrect classes is made~[\cite{liu2002performance}]. A very useful piece of information is clear citation of the base-line source, which is almost always there but lost in a sea of references.

%Should we report also incorrect or ambiguous? Could some ``correct'' be masking ambiguous? Were the ambiguous due to noise? Was the noise added on purpose? 

As we are proposing spike based data-sets, it's desirable that the users specify if there was any preprocessing applied before actually feeding the spike trains into their networks~[\cite{best-practice-nn-img}]. For example, if we want to use a particular set to test noisy inputs, it would be extremely useful to have a notion of the type of noise added.

%Traditionally, neural network training has been done using rate-based encoding. As new theories emerge, a

An important distinction to make is the nature of the training procedure. One example is the way training data was exposed to the network (e.g. How many times each image is presented? How much time is a single example shown?. [\cite{unsup_leraning_diehl}]) Also, details on the particulars of the implementation of the learning rule used (e.g. Delta, STDP, BCM, etc.) is highly desirable. 

One the biggest distinctions on learning procedures is whether they were done using some \emph{supervision} or not; making this distinction clear is vastly appreciated. On supervised learning, the label of the data influences to establish categories and connection weights. Unsupervised learning has fewer constraints when it comes to class creation but might be tougher to get right. 

A number of different classes are expected, this quantity might give an insight onto the network topology and dynamics. A description of the methods used to generate and populate the classes is very helpful for the reader. (e.g. Did we use a statistical measure? Was it a combination of NN with some other algorithms?)


\subsection{Hardware-Specific}
In order to fairly compare algorithms, hardware specifications are of utmost importance, specially when comparing times. If we are working on robotics applications, power consumption is a critical aspect, it is desirable to report metrics on the subject.

%for example, a server farm may be used for other multiple types of computing, though it might cost too much in terms of both money and energy. GPUs generally provide amazing parallel capabilities but at the cost of high power consumption. Neuromorphic hardware tends to be energy-efficient but, mainly, under development.

GPU, neuromorphic, server farm

  \subsubsection{Training Time}
    Sometimes it's easy to get confused by timing figures and what they mean. 
    Simulation time vs. real time
    From fast experimentation to commercial products
    Impact on power consumption
  \subsubsection{Latency}
    Real-time applications
    Data analysis
  \subsubsection{Power Consumption}
    Easier to experiment if not consuming a small city's power
    Performance per watt?
    Requirement for mobile platforms/robotics
    
%table summary?
