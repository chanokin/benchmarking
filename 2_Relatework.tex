\section{Related Work}
\label{sec:Related}
\subsection{Vision Literature: Related Work}
\subsubsection{MNIST~[\cite{lecun_gradient-based_1998}]}
The MNIST database was constructed from NIST's Special Database 3 and Special Database 1 which contain binary images of handwritten digits. NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.

The MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint.

SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.

Many methods have been tested with this training set and test set. Here are a few examples. Details about the methods are given in an upcoming paper. Some of those experiments used a version of the database where the input images where deskewed (by computing the principal axis of the shape that is closest to the vertical, and shifting the lines so as to make it vertical). In some other experiments, the training set was augmented with artificially distorted versions of the original training samples. The distortions are random combinations of shifts, scaling, skewing, and compression. 

\subsubsection{ImageNet~[\cite{deng_imagenet:_2009}]}
 What is ImageNet?
ImageNet is an image dataset organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a "synonym set" or "synset". There are more than 100,000 synsets in WordNet, majority of them are nouns (80,000+). In ImageNet, we aim to provide on average 1000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated. In its completion, we hope ImageNet will offer tens of millions of cleanly sorted images for most of the concepts in the WordNet hierarchy.
Why ImageNet?
The ImageNet project is inspired by a growing sentiment in the image and vision research field – the need for more data. Ever since the birth of the digital era and the availability of web-scale data exchanges, researchers in these fields have been working hard to design more and more sophisticated algorithms to index, retrieve, organize and annotate multimedia data. But good research needs good resource. To tackle these problem in large-scale (think of your growing personal collection of digital images, or videos, or a commercial web search engine’s database), it would be tremendously helpful to researchers if there exists a large-scale image database. This is the motivation for us to put together ImageNet. We hope it will become a useful resource to our research community, as well as anyone whose research and education would benefit from using a large image database. 
%Current http://www.image-net.org/
Currently there are 14,197,122 images, 21841 synsets indexed in the dataset. 
\subsubsection{Microsoft COCO~[\cite{lin_microsoft_2014}]}
We introduce a new large-scale dataset that addresses
three core research problems in scene understanding: de-
tecting non-iconic views (or non-canonical perspectives
[12]) of objects, contextual reasoning between objects
and the precise 2D localization of objects. For many
categories of objects, there exists an iconic view. For
example, when performing a web-based image search
for the object category “bike,” the top-ranked retrieved
examples appear in profile, unobstructed near the cen-
ter of a neatly composed photo. We posit that current
recognition systems perform fairly well on iconic views,
but struggle to recognize objects otherwise – in th
background, partially occluded, amid clutter [13] – re-
flecting the composition of actual everyday scenes. We
verify this experimentally; when evaluated on everyday
scenes, models trained on our data perform better than
those trained with prior datasets. A challenge is finding
natural images that contain multiple objects. The identity
of many objects can only be resolved using context, due
to small size or ambiguous appearance in the image. To
push research in contextual reasoning, images depicting
scenes [3] rather than objects in isolation are necessary.
Finally, we argue that detailed spatial understanding of
object layout will be a core component of scene analysis.
An object’s spatial location can be defined coarsely using
a bounding box [2] or with a precise pixel-level segmen-
tation [14], [15], [16]. As we demonstrate, to measure
either kind of localization performance it is essential
for the dataset to have every instance of every object2
category labeled and fully segmented. Our dataset is
unique in its annotation of instance-level segmentation
masks.
\subsubsection{YouTube Action Dataset~[\cite{liu_recognizing_2009}]}
In this paper, we present a systematic framework for recognizing realistic actions from videos “in the wild”. Such unconstrained videos are abundant in personal collections as well as on the Web. Recognizing action from such videos has not been addressed extensively, primarily due to the tremendous variations that result from camera motion, background clutter, changes in object appearance, and scale, etc. The main challenge is how to extract reliable and informative features from the unconstrained videos. We extract both motion and static features from the videos. Since the raw features of both types are dense yet noisy, we propose strategies to prune these features. We use motion statistics to acquire stable motion features and clean static features. Furthermore, PageRank is used to mine the most informative static features. In order to further construct compact yet discriminative visual vocabularies, a divisive information-theoretic algorithm is employed to group semantically related features. Finally, AdaBoost is chosen to integrate all the heterogeneous yet complementary features for recognition. We have tested the framework on the KTH dataset and our own dataset consisting of 11 categories of actions collected from YouTube and personal videos, and have obtained impressive results for action recognition and action localization.

Two early benchmarks are the KTH [32] and Weiz-
mann [1] sets, both used extensively over the years. These
sets provide low resolution videos of a few, “atomic” action
categories, such as walking, jogging, and running. These
videos were produced “in the lab”, and present actors that
perform scripted behavior. The videos they provide were
acquired under controlled conditions, with static cameras
and static, un-clutered backgrounds. In addition, actors ap-
pear without occlusions, thus allowing action recognition to
be performed by considering silhouettes alone [1].

In an effort to increase the diversity of appearances and
viewing conditions, benchmark designers have turned to
TV, sports broadcasts and motion pictures as sources for
challenging videos of human actions. These benchmarks
no longer represent controlled conditions; viewpoints, illu-
minations, occlusions are all arbitrary, thereby significantly
raising the bar for action recognition systems.
One early example is the UIUC2 benchmark [36], which
provided unconstrained sports videos of badminton matches
downloaded from YouTube. To our knowledge, this is the
first benchmark to provide such unconstrained data. An-
other early, popular example is the UCF-Sports bench-
mark [28] with its 200 videos of nine different sports events,
collected from TV broadcasts. Sports videos were also con-
sidered in the Olympic-Games benchmark of [23].

\subsection{Current Status}
what are the problems the current database aim for.
Current methods/algorithms existing.


The FERET tests are administered using a testing proto-
col, which states the mechanics of the tests and the manner
in which the test will be scored. In face recognition, for
example, the protocol states the number of images of each
person in the test, how the output from the algorithm is
recorded, and how the performance results are reported.
There is a direct connection among the problem being
evaluated, the images in the database, and the testing pro-
tocol. The testing protocol and the images define the pro-
blem to be evaluated. The characteristics and quality of the
images are major factors in determining the difficulty of the
problem. For example, if the faces are in a predetermined
position in the images, the problem is different from that for
images in which the faces can be located anywhere in the
image. In the FERET database, variability was introduced
by the inclusion of images taken at different dates and loca-
tions (see Section 4.2). This resulted in changes in lighting,
scale, and background.
The goals for the FERET evaluation process were to assess
the state of the art, advance the state of the art, and point to
future directions of research. Accomplishing all three goals
was a delicate process, and the keys to success were the
database and the tests. If algorithms existed that could easily
solve the problem, then the evaluation process would be
reduced to ‘tuning’ existing algorithms. On the other hand,
if the images defined a problem that was beyond current
algorithmic techniques, then the results would have been
poor and would have not allowed an accurate assessment of
current algorithmic capabilities. The key was to find the right
balance, so that if the problem formulated could not be solved
satisfactorily by existing methods, it would be possible to
develop algorithms that could solve it.
The collection of the FERET database was initiated in
September 1993, and the first FERET test was administered
in August 1994. A standard database of facial images was
established in the first year and made available to research-
ers; this database provided the images for the Aug94
FERET test. (Throughout this article, date-based names
such as Aug94 are used to refer to the same FERET test,
even when the tests were administered on other dates.) The
Aug94 FERET test established a performance baseline for
fully automatic face-recognition
algorithms. A fully auto-
matic algorithm does not require the location of the face in
the image as input: the algorithm locates and identifies the
face in the image.
The Aug94 FERET test was designed to measure the
performance of algorithms that could automatically locate,
normalize, and identify faces from a database. The test
consisted of three subsets: the large gallery, false alarm,
and rotation tests. The first tested the ability of algorithms
to recognize faces from a set of 317 known individuals
(referred to as the gallery; an image of an unknown face
presented to the algorithm is a probe, and the collection of
probes is called the probe set). The second subtest, the false-
alarm test, measured how well an algorithm rejects faces not
in the gallery. The third baselined the effects of pose
changes (rotations) on performance. On the basis of the
Aug94 FERET test, it was concluded that algorithms needed
to be evaluated on (1) larger galleries and (2) a substantially
increased number of duplicate probes. (A duplicate is
defined as an image of a person whose corresponding
gallery image was taken on a different date.)
A second FERET test, first administered in March 1995
(and referred to as the Mar95 FERET test), was designed
based on the conclusions from the Aug94 FERET test. The
Mar95 FERET test evaluated algorithms on larger galleries
and probe sets with a greater number of duplicates. This
required that additional images be collected, with an emphasis
on images of the same people taken months or years apart.

\subsubsection{Converting from MPL to SNN}
It remains a challenge to transform traditional artificial neural networks into spiking ones.
There are attempts~\cite{la2008response}~\cite{burkitt2006review} to estimate the output firing rate of the LIF neurons (Equation~\ref{equ:lif}) under certain conditions. 
%For the model illustrated above, there are two types of synaptic connection: one-to-one connections in the retina layer and N-to-one connections in all the convolutional layers (the pooling layer is also included). 
%For the retina layer, 1) the problem is: what is the connection weight between two single LIF neurons to make a post-synaptic neuron fire whenever the pre-synaptic neuron generates a spike? 
%While for the convolutional neurons, 2) given the input spike rates, LIF neuron parameters and the output spiking rate, what are the corresponding weights between the two layers?
\begin{equation}
\frac{\D \: V(t)}{\D\:  t}=-\frac{V(t)-V_\mathit{rest}}{\tau_m}+\frac{I(t)}{C_m}
\label{equ:lif}
\end{equation}
The membrane potential $V$ changes in response to input current $I$, starting at the resting membrane potential  $V_{rest}$, where the membrane time constant is $\tau_m = R_mC_m$, $R_m$ is the membrane resistance and $C_m$ is the membrane capacitance.

Given a constant current injection $I$, the response function, i.e. firing rate, of the LIF neuron is
\begin{equation}
\lambda_\mathit{out}=
\left [ t_\mathit{ref}-\tau_m\ln \left ( 1-\frac{V_{th}-V_\mathit{rest}}{IR_m}  \right )\right ]^{-1}
\label{equ:consI}
\end{equation}
when $IR_m>V_{th}-V_{rest}$, otherwise the membrane potential cannot reach the threshold $V_{th}$ and the output firing rate is zero. 
The absolute refractory period $t_\mathit{ref}$ is included, where all input during this period is invalid.
In a more realistic scenario, the post-synaptic potentials (PSPs) are triggered by the spikes generated from the neuron's pre-synaptic neurons other than a constant current.
Assume that the synaptic inputs are Poisson spike trains, the membrane potential of the LIF neuron is considered as a diffusion process. Equation~\ref{equ:lif} can be modelled as a stochastic differential equation referring to Ornstein-Uhlenbeck process,
\begin{equation}
\tau_m\frac{\D\:V(t)}{\D\:  t}=-\left[V(t)-V_\mathit{rest}\right] + \mu + \sigma\sqrt{2\tau_m}\xi (t)
\label{equ:sde}
\end{equation}
where
\begin{equation}
\begin{array}{l}
\mu=\tau_m(\mathbf{w_E\cdot\lambda_E}-\mathbf{w_I\cdot\lambda_I})
\\
\\
\sigma ^{2} = \frac{\tau_m}{2}\left(\mathbf{w_E^{2}\cdot\lambda_E}+\mathbf{w_I^{2}\cdot\lambda_I}\right)
\end{array}
\label{equ:ou}
\end{equation}
are the conditional mean and variance of the membrane potential.
The delta-correlated process $\xi(t)$ is Gaussian white noise with zero mean, $\mathbf{w_E}$ and $\mathbf{w_I}$ stand for the weight vectors of the excitatory and the inhibitory synapses, and $\mathbf{\lambda}$ represents the vector of the input firing rate.
The response function of the LIF neuron with Poisson input spike trains is given by the Siegert function~\cite{siegert1951first}, 
%\begin{equation}
%%\lambda_\mathit{out}=\left(\tau_\mathit{ref} + \frac{\tau_Q}{\sigma_Q}\sqrt{\frac{\pi}{2}} \int_{V_\mathit{rest}}^{V_\mathit{th}}du \:\exp \left(\frac{u-\mu_Q}{\sqrt2\sigma_Q} \right )^{2}\left[1+erf \left(\frac{u-\mu_Q}{\sqrt2\sigma_Q} \right ) \right ]\right)^{-1}
%\begin{split}
%\lambda_\mathit{out}=\left(\tau_\mathit{ref} + \frac{\tau_Q}{\sigma_Q}\sqrt{\frac{\pi}{2}} \int_{V_\mathit{rest}}^{V_\mathit{th}}\D u \:\exp \left(\frac{u-\mu_Q}{\sqrt2\sigma_Q} \right )^{2}\left[1+\mathrm{erf} \left(\frac{u-\mu_Q}{\sqrt2\sigma_Q} \right ) \right ]\right)^{-1}
%\end{split}
%\label{equ:sgt}
%\end{equation}
\begin{align}
\lambda_\mathit{out} &=\left(\tau_\mathit{ref} + \frac{\tau_Q}{\sigma_Q}\sqrt{\frac{\pi}{2}} \int_{V_\mathit{rest}}^{V_\mathit{th}}\D\,u \:\exp \left(\frac{u-\mu_Q}{\sqrt2\sigma_Q} \right )^{2} \right. \nonumber \\
&\qquad \left. \vphantom{\int_t} \cdot  \left[1+\mathrm{erf} \left(\frac{u-\mu_Q}{\sqrt2\sigma_Q} \right ) \right ]\right)^{-1}
\label{equ:sgt}
\end{align}
where $\tau_Q, \mu_Q, \sigma_Q$ are identical to $\tau_m, \mu, \sigma$ in Equation~\ref{equ:ou}, and erf is the error function.

Still there are some limitations on the response function. 
For the diffusion process, only small amplitude (weight) of the PostSynaptic Potentials (PSPs) generated by a large amount of input spikes (high spiking rate) work under this circumstance; 
plus, the delta function is required, i.e. the synaptic time constant is considered to be zero. Thus only a rough approximation of the output spike rate has been determined.
Secondly, given different input spike rate to each pre-synaptic neurons, the parameters of the LIF neuron and the output spiking rate, how to tune every single corresponding synaptic weight remains a difficult task.

\subsubsection{Rank-Order-Coding}

\subsubsection{Liquid State Machine/Reservoir Encoding}
