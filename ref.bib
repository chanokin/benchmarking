@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {0018-9219},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	keywords = {2D shape variability, backpropagation, back-propagation, character recognition, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition, document recognition systems, Feature extraction, field extraction, gradient-based learning, gradient based learning technique, graph transformer networks, GTN, handwritten character recognition, handwritten digit recognition task, hidden Markov models, high-dimensional patterns, language modeling, machine learning, Multi-layer neural network, multilayer neural networks, multilayer perceptrons, multimodule systems, Neural networks, optical character recognition, Optical character recognition software, Optical computing, Pattern Recognition, performance measure minimization, Principal component analysis, segmentation recognition},
	pages = {2278--2324},
	file = {IEEE Xplore Abstract Record:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/RDDSDJHU/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/G3CT5B75/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf}
}
@inproceedings{deng_imagenet:_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, 2009. {CVPR} 2009},
	author = {Deng, Jia and Dong, Wei and Socher, R. and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	keywords = {computer vision, Explosions, Image databases, ImageNet database, image resolution, image retrieval, Information retrieval, Internet, large-scale hierarchical image database, large-scale ontology, Large-scale systems, multimedia computing, multimedia data, Multimedia databases, Ontologies, ontologies (artificial intelligence), Robustness, Spine, subtree, trees (mathematics), very large databases, visual databases, wordNet structure},
	pages = {248--255},
	file = {IEEE Xplore Abstract Record:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/TFFAKIMS/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/QMNG4F75/Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf:application/pdf}
}

@inproceedings{liu_recognizing_2009,
	title = {Recognizing realistic actions from videos \#x201C;in the wild \#x201D;},
	doi = {10.1109/CVPR.2009.5206744},
	abstract = {In this paper, we present a systematic framework for recognizing realistic actions from videos “in the wild”. Such unconstrained videos are abundant in personal collections as well as on the Web. Recognizing action from such videos has not been addressed extensively, primarily due to the tremendous variations that result from camera motion, background clutter, changes in object appearance, and scale, etc. The main challenge is how to extract reliable and informative features from the unconstrained videos. We extract both motion and static features from the videos. Since the raw features of both types are dense yet noisy, we propose strategies to prune these features. We use motion statistics to acquire stable motion features and clean static features. Furthermore, PageRank is used to mine the most informative static features. In order to further construct compact yet discriminative visual vocabularies, a divisive information-theoretic algorithm is employed to group semantically related features. Finally, AdaBoost is chosen to integrate all the heterogeneous yet complementary features for recognition. We have tested the framework on the KTH dataset and our own dataset consisting of 11 categories of actions collected from YouTube and personal videos, and have obtained impressive results for action recognition and action localization.},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, 2009. {CVPR} 2009},
	author = {Liu, Jingen and Luo, Jiebo and Shah, M.},
	month = jun,
	year = {2009},
	keywords = {action localization, AdaBoost, cameras, computer vision, Feature extraction, Humans, image motion analysis, information-theoretic algorithm, informative static features, KTH dataset, motion features, Motion pictures, motion statistics, PageRank, personal videos, realistic action recognition, Shape, spatiotemporal phenomena, unconstrained videos, Videos, video signal processing, visual vocabularies, Vocabulary, YouTube},
	pages = {1996--2003},
	file = {IEEE Xplore Abstract Record:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/DVK7SNJD/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/liuq/.mozilla/firefox/m2i8ko12.default/zotero/storage/7VFBCHUQ/Liu et al. - 2009 - Recognizing realistic actions from videos #x201C\;i.pdf:application/pdf}
}