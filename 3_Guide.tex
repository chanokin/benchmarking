\section{Guiding Principles}
\label{sec:guide}
\subsection{The Goals}
With this benchmark we hope to (1) promote meaningful comparison among algorithms in the field of neural computation, (2) allow comparison with conventional image recognition methods, (3) provide an assessment of the state of the art in spike-based visual recognition, and (4) help researchers identify future directions and advance the field.
\subsection{Dataset and Testing Protocols Referring the Goals}

%The FERET database was established to support both
%algorithm development and evaluation. Two guiding prin-
%ciples were followed. First, the evaluation of algorithms
%requires a common database of images for both develop-
%ment and testing. In the FERET evaluation, the images in
%the test are from both the development
%and sequestered
%portions of the FERET database. Second, the variety and
%difficulty of the problems defined by the images in the data-
%base must increase incrementally.
%The need to test algorithms against a database is obvious,
%but the development
%function of the database is equally
%important (if less obvious). For the evaluation procedure
%to produce meaningful results, the images in the develop-
%mental portion of the database must resemble those on
%which algorithms are to be tested. The development and
%testing data sets must be similar in both quality and quantity.
%For example, if the test will consist of a gallery of 1000
%individuals,
%it is not appropriate
%for the development
%database to consist of 50 individuals. The algorithms tested
%will be only as good as the database from which they are
%developed. The FERET evaluation procedure followed this
%principle by partitioning the FERET database into the devel-
%opmental and sequestered portions, where the developmen-
%tal portion was representative
%of the sequestered portion
%(details are provided in Section 4.2).
%The other principle is that the problem defined by the
%images in the database must mesh with the current level
%of algorithm development, and the difficulty of the database
%must grow as the sophistication of the algorithms increases.
%As explained in Section 2, if the database defines a problem
%that is too easy, testing the algorithm becomes a mere tuning
%exercise. At the other extreme, if the problem is too far
%beyond the state of the art, the test will not produce any
%meaningful results. To prevent the FERET database from
%becoming stale, we continuously expanded and adjusted the
%database to the state of the art in face recognition.

\subsection{The Proposal: Advantages}
In this paper we present a comprehensive method of eval-
uating face-recognition
algorithms, developed as part of the
Face Recognition Technology (FERET) program [ 10,111.
The FERET evaluation methodology consists of an inte-
grated data collection effort and testing program. These
two parts are integrated through the FERET database of
facial images; the database is divided into a development
portion, which is provided to researchers, and a sequestered
portion for testing. The partition of the database enables
algorithms to be trained and tested on different, but related,
sets of images. The FERET evaluation procedure is a set of
standard testing protocols: the FERET tests are indepen-
dently administered
and each test is completed within
three days. The use of a standard testing protocol allows
for the direct comparison of algorithms developed by dif-
ferent groups, as well as for measuring improvements made
by any single group over time.

