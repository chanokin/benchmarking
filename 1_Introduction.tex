\section{Introduction}
\label{sec:intro}
\subsection{What Is the Problem}
In the last few years, vision based spiking neural networks(SNNs) has become an active area of computer vision.
The interest has been fuelled by potential applications and by the simultaneous development of algorithmic techniques and inexpensive computers with the computational power to run these algorithms.
These developments yielded a large number of papers on SNNs based recognition, with a majority reporting outstanding recognition results (usually $>$ 95\% correct recognition) on limited-size databases (usually $<$ 50 individuals).
%Reported results:
%Hand Gestures, Qian Liu
Inspired by the behaviours of the primary visual cortex, Convolutional Neural Networks (CNNs) are modelled using
both linear perceptrons and spiking Leaky Integrate-and-Fire (LIF) neurons. In this study's largest configuration using these
approaches, a network of 74,210 neurons and 15,216,512 synapses is created and operated in real-time using 290 SpiNNaker
processor cores in parallel and with 93.0% accuracy. A smaller network using only 1/10th of the resources is also created,
again operating in real-time, and it is able to recognize the postures with an accuracy of around 86.4% - only 6.6% lower than
the much larger system. The recognition rate of the smaller network developed on this neuromorphic system is sufficient for a
successful hand posture recognition system, and demonstrates a much improved cost to performance trade-off in its approach.
%Hand Gestures, Lee
The motion trajectories of moving hands are detected by spatiotemporally correlating the stereoscopically verged asynchronous events from the DVSs by using leaky integrate-and-fire (LIF) neurons. Adaptive thresholds of the LIF neurons achieve the segmentation of trajectories, which are then translated into discrete and finite feature vectors. The feature vectors are classified with hidden Markov models, using a separate Gaussian mixture model for spotting irrelevant transition gestures. The disparity information from stereovision is used to adapt LIF neuron parameters to achieve recognition invariant of the distance of the user to the sensor, and also helps to filter out movements in the background of the user. Exploiting the high dynamic range of DVSs, furthermore, allows gesture recognition over a 60-dB range of scene illuminance. The system achieves recognition rates well over 90\% under a variety of variable conditions with static and dynamic backgrounds with naïve users.
%MNSIT Emre Neftci
%The recurrent activity of the network replaces the discrete steps of the CD algorithm, while Spike Time Dependent Plasticity (STDP) carries out the weight updates in an online, asynchronous fashion. We demonstrate our approach by training an RBM composed of leaky I&F neurons with STDP synapses to learn a generative model of the MNIST hand-written digit dataset, and by testing it in recognition, generation and cue integration tasks. Our results contribute to a machine learning-driven approach for synthesizing networks of spiking neurons capable of carrying out practical, high-level functionality.
%MNIST Evangelos Stromatias
In  this  context  we  in-
troduce  a  realization  of  a  spike-based  variation  of  previously
trained  DBNs  on  the  biologically-inspired  parallel  SpiNNaker
platform. The DBN on SpiNNaker runs in real-time and achieves
a classification performance of 95% on the MNIST handwritten
digit  dataset,  which  is  only  0.06%  less  than  that  of  a  pure
software implementation. Importantly, using a neurally-inspired
architecture yields additional benefits: during network run-time
on this task, the platform consumes only 0.3 W with classification
latencies in the order of tens of milliseconds, making it suitable
for  implementing  such  networks  on  a  mobile  platform.  The
results in this paper also show how the power dissipation of the
SpiNNaker platform and the classification latency of a network
scales with the number of neurons and layers in the network and
the overall spike activity rate.


Few of these algorithms reported results on images from a common database; fewer met the desirable goal of being evaluated against a standard testing protocol that includes separate training and testing sets.
As a consequence, there was no way to make a quantitative assessment of the algorithms' relative strengths and weaknesses.
Unfortunately, this is not an isolated case, but an endemic problem in computer vision research.
\subsection{Vision Literature: Related Work}
\subsubsection{MNIST~[\cite{lecun_gradient-based_1998}]}
The MNIST database was constructed from NIST's Special Database 3 and Special Database 1 which contain binary images of handwritten digits. NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.

The MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint.

SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.

Many methods have been tested with this training set and test set. Here are a few examples. Details about the methods are given in an upcoming paper. Some of those experiments used a version of the database where the input images where deskewed (by computing the principal axis of the shape that is closest to the vertical, and shifting the lines so as to make it vertical). In some other experiments, the training set was augmented with artificially distorted versions of the original training samples. The distortions are random combinations of shifts, scaling, skewing, and compression. 

\subsubsection{ImageNet~[\cite{deng_imagenet:_2009}]}
 What is ImageNet?
ImageNet is an image dataset organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a "synonym set" or "synset". There are more than 100,000 synsets in WordNet, majority of them are nouns (80,000+). In ImageNet, we aim to provide on average 1000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated. In its completion, we hope ImageNet will offer tens of millions of cleanly sorted images for most of the concepts in the WordNet hierarchy.
Why ImageNet?
The ImageNet project is inspired by a growing sentiment in the image and vision research field – the need for more data. Ever since the birth of the digital era and the availability of web-scale data exchanges, researchers in these fields have been working hard to design more and more sophisticated algorithms to index, retrieve, organize and annotate multimedia data. But good research needs good resource. To tackle these problem in large-scale (think of your growing personal collection of digital images, or videos, or a commercial web search engine’s database), it would be tremendously helpful to researchers if there exists a large-scale image database. This is the motivation for us to put together ImageNet. We hope it will become a useful resource to our research community, as well as anyone whose research and education would benefit from using a large image database. 
%Current http://www.image-net.org/
Currently there are 14,197,122 images, 21841 synsets indexed in the dataset. 
\subsubsection{Microsoft COCO~[\cite{lin_microsoft_2014}]}
We introduce a new large-scale dataset that addresses
three core research problems in scene understanding: de-
tecting non-iconic views (or non-canonical perspectives
[12]) of objects, contextual reasoning between objects
and the precise 2D localization of objects. For many
categories of objects, there exists an iconic view. For
example, when performing a web-based image search
for the object category “bike,” the top-ranked retrieved
examples appear in profile, unobstructed near the cen-
ter of a neatly composed photo. We posit that current
recognition systems perform fairly well on iconic views,
but struggle to recognize objects otherwise – in th
background, partially occluded, amid clutter [13] – re-
flecting the composition of actual everyday scenes. We
verify this experimentally; when evaluated on everyday
scenes, models trained on our data perform better than
those trained with prior datasets. A challenge is finding
natural images that contain multiple objects. The identity
of many objects can only be resolved using context, due
to small size or ambiguous appearance in the image. To
push research in contextual reasoning, images depicting
scenes [3] rather than objects in isolation are necessary.
Finally, we argue that detailed spatial understanding of
object layout will be a core component of scene analysis.
An object’s spatial location can be defined coarsely using
a bounding box [2] or with a precise pixel-level segmen-
tation [14], [15], [16]. As we demonstrate, to measure
either kind of localization performance it is essential
for the dataset to have every instance of every object2
category labeled and fully segmented. Our dataset is
unique in its annotation of instance-level segmentation
masks.
\subsubsection{YouTube Action Dataset~[\cite{liu_recognizing_2009}]}
In this paper, we present a systematic framework for recognizing realistic actions from videos “in the wild”. Such unconstrained videos are abundant in personal collections as well as on the Web. Recognizing action from such videos has not been addressed extensively, primarily due to the tremendous variations that result from camera motion, background clutter, changes in object appearance, and scale, etc. The main challenge is how to extract reliable and informative features from the unconstrained videos. We extract both motion and static features from the videos. Since the raw features of both types are dense yet noisy, we propose strategies to prune these features. We use motion statistics to acquire stable motion features and clean static features. Furthermore, PageRank is used to mine the most informative static features. In order to further construct compact yet discriminative visual vocabularies, a divisive information-theoretic algorithm is employed to group semantically related features. Finally, AdaBoost is chosen to integrate all the heterogeneous yet complementary features for recognition. We have tested the framework on the KTH dataset and our own dataset consisting of 11 categories of actions collected from YouTube and personal videos, and have obtained impressive results for action recognition and action localization.

Two early benchmarks are the KTH [32] and Weiz-
mann [1] sets, both used extensively over the years. These
sets provide low resolution videos of a few, “atomic” action
categories, such as walking, jogging, and running. These
videos were produced “in the lab”, and present actors that
perform scripted behavior. The videos they provide were
acquired under controlled conditions, with static cameras
and static, un-clutered backgrounds. In addition, actors ap-
pear without occlusions, thus allowing action recognition to
be performed by considering silhouettes alone [1].

In an effort to increase the diversity of appearances and
viewing conditions, benchmark designers have turned to
TV, sports broadcasts and motion pictures as sources for
challenging videos of human actions. These benchmarks
no longer represent controlled conditions; viewpoints, illu-
minations, occlusions are all arbitrary, thereby significantly
raising the bar for action recognition systems.
One early example is the UIUC2 benchmark [36], which
provided unconstrained sports videos of badminton matches
downloaded from YouTube. To our knowledge, this is the
first benchmark to provide such unconstrained data. An-
other early, popular example is the UCF-Sports bench-
mark [28] with its 200 videos of nine different sports events,
collected from TV broadcasts. Sports videos were also con-
sidered in the Olympic-Games benchmark of [23].


\subsection{The Proposal: Advantages}
In this paper we present a comprehensive method of eval-
uating face-recognition
algorithms, developed as part of the
Face Recognition Technology (FERET) program [ 10,111.
The FERET evaluation methodology consists of an inte-
grated data collection effort and testing program. These
two parts are integrated through the FERET database of
facial images; the database is divided into a development
portion, which is provided to researchers, and a sequestered
portion for testing. The partition of the database enables
algorithms to be trained and tested on different, but related,
sets of images. The FERET evaluation procedure is a set of
standard testing protocols: the FERET tests are indepen-
dently administered
and each test is completed within
three days. The use of a standard testing protocol allows
for the direct comparison of algorithms developed by dif-
ferent groups, as well as for measuring improvements made
by any single group over time.

First of all, an initial dataset of input stimuli based on standard computer vision benchmarks consisting of %facial images (FERET database) and 
digits (MNIST database) is presented according to the current research on spike-based image recognition.
Within this dataset, all images are centre aligned and with similar scale.
We describe how we intend to expand this dataset to fulfil the needs of upcoming research problems.
For instance, the data should provide cases to measure position-, scale-, and viewing-angle invariance.
The data will be in Address-Event Representation (AER) format which is well-applied in neuromorphic engineering field unlike conventional images.
These spike trains are produced by various techniques: rate-based Poisson spike generation, rank order encoding and recorded output from a silicon retina with both flashing and oscillating input stimuli.
An evaluation methodology is also presented which describes how to consistently assess the accuracy, speed, efficiency and cost of an algorithm working with the dataset.
Finally, we provide a baseline for comparison based on a proposed SNN's performance on the dataset.
The network is trained on-line using the Spike Timing Dependent Plasticity (STDP) learning rule on a massive-parallel neuromorphic simulator, e.g. SpiNNaker.