\section{Performance Evaluation}
\label{sec:eval}

A crucial part of research is reporting results and comparing achievements with other state-of-the-art work. Unfortunately there is no standard way of fulfilling that task, this leads to a hard 
We would like to assist with the following considerations.
<<<<<<< HEAD


\subsection{Hardware-Independent}
A brief description of the \emph{network topology} is most welcome, we believe different topologies will have a deep impact on the overall performance. Furthermore, sharing this designs can inspire fellow scientists to create new structures that, might  a new point-of-view to bloom, generating a positive feedback loop where everybody wins.

Most classification papers report a percentage of accuracy that gives the reader a measure of the correct classifications~[\cite{dietterich1998approximate}]. Some times it might be desirable, for a better understanding of the paper, that a distinction between ambiguous, outliers and incorrect classes is made~[\cite{liu2002performance}]. A very useful piece of information is clear citation of the base-line source, which is almost always there but lost in a sea of references.

%Should we report also incorrect or ambiguous? Could some ``correct'' be masking ambiguous? Were the ambiguous due to noise? Was the noise added on purpose? 

As we are proposing spike based data-sets, it's desirable that the users specify if there was any preprocessing applied before actually feeding the spike trains into their networks~[\cite{best-practice-nn-img}]. For example, if we want to use a particular set to test noisy inputs, it would be extremely useful to have a notion of the type of noise added.

%Traditionally, neural network training has been done using rate-based encoding. As new theories emerge, a

An important distinction to make is the nature of the training procedure. One example is the way data was exposed to the network (e.g. How many times each image was presented? How much time was a single example shown?. [\cite{unsup_leraning_diehl}]) Also, details on the particulars of the implementation of the learning rule used (e.g. Delta, STDP, BCM, etc.) is highly desirable. 

One the biggest distinctions on learning procedures is whether they were done using some \emph{supervision} or not; making this distinction clear is vastly appreciated. On supervised learning, the label of the data influences to establish categories and connection weights. Unsupervised learning has fewer constraints when it comes to class creation but might be tougher to get right. 

A number of different classes are expected, this quantity might give an insight onto the network topology and dynamics. A description of the methods used to generate and populate the classes is very helpful for the reader. (e.g. Did we use a statistical measure? Was it a combination of NN with some other algorithms?)

\begin{table*}
  \caption{Model comparison}
=======
\begin{table*}[hbt!]
  \caption{Hardware independent comparison}
>>>>>>> 30def41e265e1f888c75748e7dad3dd29d290fc4
  \begin{center}
    \bgroup
    \def\arraystretch{1.4}
    \begin{tabular}{ l | c c c c c c }
      $ $ &
      \begin{minipage}{1.9cm}\centering Topology \end{minipage} & 
      \begin{minipage}{1.9cm}\centering Accuracy \end{minipage} & 
      \begin{minipage}{1.9cm}\centering Preprocessing \end{minipage} &
      \begin{minipage}{1.9cm}\centering Training \end{minipage} & 
      \begin{minipage}{1.9cm}\centering Supervised \end{minipage} &
      \begin{minipage}{1.9cm}\centering Extra classifier \end{minipage} \\
      \hline
%contents
      \begin{minipage}{2cm} Paper 1 [ref] \end{minipage}  & & & & & & \\
      \begin{minipage}{2cm} Paper 2 [ref]\end{minipage}  & & & & & & \\
      \begin{minipage}{2cm} Paper 3 [ref]\end{minipage}  & & & & & & \\
      \begin{minipage}{2cm} Paper 4 [ref]\end{minipage}  & & & & & & 
    \end{tabular}
    \egroup
  \end{center}
  \label{tb:software_comparison}
\end{table*}

\subsection{Hardware-Independent}
A brief description of the \emph{network topology} is most welcome, we believe different topologies will have a deep impact on the overall performance. Furthermore, sharing this designs can inspire fellow scientists to create new structures that, might  a new point-of-view to bloom, generating a positive feedback loop where everybody wins.

Most classification papers report a percentage of accuracy that gives the reader a measure of the correct classifications~[\cite{dietterich1998approximate}]. Some times it might be desirable, for a better understanding of the paper, that a distinction between ambiguous, outliers and incorrect classes is made~[\cite{liu2002performance}]. A very useful piece of information is clear citation of the base-line source, which is almost always there but lost in a sea of references.

%Should we report also incorrect or ambiguous? Could some ``correct'' be masking ambiguous? Were the ambiguous due to noise? Was the noise added on purpose? 

As we are proposing spike based data-sets, it's desirable that the users specify if there was any preprocessing applied before actually feeding the spike trains into their networks~[\cite{best-practice-nn-img}]. For example, if we want to use a particular set to test noisy inputs, it would be extremely useful to have a notion of the type of noise added.

%Traditionally, neural network training has been done using rate-based encoding. As new theories emerge, a

An important distinction to make is the nature of the training procedure. One example is the way data was exposed to the network (e.g. How many times each image was presented? How much time was a single example shown?. [\cite{Diehl2015unsupervised}]) Also, details on the particulars of the implementation of the learning rule used (e.g. Delta, STDP, BCM, etc.) is highly desirable. 

One the biggest distinctions on learning procedures is whether they were done using some \emph{supervision} or not; making this distinction clear is vastly appreciated. On supervised learning, the label of the data influences to establish categories and connection weights. Unsupervised learning has fewer constraints when it comes to class creation but might be tougher to get right. 

A number of different classes are expected, this quantity might give an insight onto the network topology and dynamics. A description of the methods used to generate and populate the classes is very helpful for the reader. (e.g. Did we use a statistical measure? Was it a combination of NN with some other algorithms?)




\subsection{[Evangelos Stromatias] Hardware-Specific}


Depending on how neurons, synapses and spike transmission are implemented neuromorphic systems can be categorised as either analogue, digital, or mixed-mode analogue/digital VLSI circuits. Analogue implementations exploit the sub-threshold transistor dynamics to emulate neurons and synapses directly on hardware \citep{giacom} and are more energy-efficient while requiring less area than their digital counterparts \citep{temamanalogdigital}. However, the behaviour of analogue circuits is largely determined during the fabrication process due to transistor mismatch \citep{giacom,analoguemismatch,bernabeDACsynapses}, while their wiring densities render them impractical for large-scale systems. The majority of mixed-mode analogue/digital neuromorphic platforms, such as High Input Count Analog Neural Network (HI-CANN) \citep{Schemmel_etal10}, Neurogrid \citep{Benjamin_etal14}, HiAER-IFAT \citep{gert}, use analogue circuits to emulate neurons and digital packet-based technology to communicate spikes as AER events. This enables reconfigurable connectivity patterns, while the time of spikes is expressed implicitly since typically a spike reaches its destination in less than a millisecond, thus fulfilling the real-time requirement. Digital neuromorphic platforms such as TrueNorth \citep{Merolla08082014} use digital circuits with finite precision to simulate neurons in an event driven manner to minimise the active power dissipation. Neuromorphic systems suffer from model flexibility, since neurons and synapses are fabricated directly on hardware with only a small subset of parameters exposed to the researcher. 

The Spiking Neural Network Architecture, or SpiNNaker, is a biologically inspired, massively-parallel, scalable computing architecture designed by the Advanced Processor Technologies (APT) group at the University of Manchester. SpiNNaker has been optimised to simulate very large-scale spiking neural networks in real-time \citep{spiNNakerProject}. SpiNNaker aims to combine the advantages of conventional computers and neuromorphic hardware by utilising low-power programmable cores and scalable event-driven communications hardware. \textit{\textbf{[ADD AS MUCH DETAILS ABOUT THE SPINNAKER ARCHITECTURE AND SOFTWARE HERE]}}. 

A direct comparison between neuromorphic platforms is not a trivial task due to the different hardware implementation technology. Comparing the performance of each platform in terms of power requirements is an interesting comparison metric especially if targeted for mobile applications and robotics. Some researchers have suggested the use of energy per synaptic event (SE) \citep{Sharp2012110,strometal} as a power metric because the large number of fan in and out of a neuron tend to dominate the total power dissipation during a simulation. Merolla et al. proposed the number of synaptic operations per second per Watt (SOPS/W) \citep{Merolla08082014}. 

Additional comparison metrics could be the precision used to describe the membrane potential of neurons (for the digital platforms), synaptic weights, axonal delays, simulation type (time or event driven) and if the simulation runs in real- or accelerated time.
 
Table~\ref{tb:hardware_comparison} aims to summarise the aforementioned hardware comparison metrics.
 
% mention spinnaker, for this work

%power

%real-time or accelerated

% latency

%Specifying hardware is of utmost importance when comparing computing times and power consumption. Analog or digital or a hybrid system.
%
%Different platforms have special benefits, purely hardware solutions have low power consumption but lack programmability.
%
%New theories, such as Polychronization, suggest that axonal delays are an integral part of the brain's computing mechanisms~[\cite{Izhikevich2005}]. 
%
%Power consumption is a key issue for mobile applications and robotics. A way to measure is to state the number of \emph{Synaptic operations per Watt} that the hardware is capable of.
%
%An important factor to measure performance is the number of \emph{Synaptic events per second}; i.e. rough throughput
%
%A piece of hardware that is difficult to use/program is of little use, thus \emph{front-end support} is
%
%Neural activity highly depends on synapses, specifying what model was used and its precision will impact on the performance.

\begin{table*}
  \caption{Hardware dependent comparison}
  \begin{center}
      \bgroup
      \def\arraystretch{1.4}
    \begin{tabular}{l | c c c c c c c c c}
      $ $ & 
       \begin{minipage}{1.2cm}\centering Hardware approach \end{minipage} & 
       \begin{minipage}{1.3cm}\centering Simulation type \end{minipage} & 
       \begin{minipage}{1.7cm}\centering Programmable \end{minipage} & 
       \begin{minipage}{1cm}\centering Axonal delays \end{minipage} & 
       \begin{minipage}{1cm}\centering Synaptic model \end{minipage} & 
       \begin{minipage}{1.2cm}\centering Synaptic precision \end{minipage} & 
<<<<<<< HEAD
       \begin{minipage}{1.4cm}\centering Synaptic ops per sec \end{minipage} & 
       \begin{minipage}{1.6cm}\centering Synaptic ops per Watt \end{minipage} & 
       \begin{minipage}{1.7cm}\centering Maximum neurons \end{minipage}  \\
=======
       \begin{minipage}{1.2cm}\centering Energy per SE \end{minipage} & 
       \begin{minipage}{1.4cm}\centering Synaptic ops per Watt \end{minipage} & 
       \begin{minipage}{1.7cm}\centering Programming front-end \end{minipage}  \\
>>>>>>> 30def41e265e1f888c75748e7dad3dd29d290fc4
       \hline
       % contents!
       \begin{minipage}{1.8cm}\centering SpiNNaker \citep{strometal} \end{minipage} &Digital& & & & & & 8~nJ &54.27 MSops/W & \\
       \begin{minipage}{1.8cm}\centering TrueNorth \citep{Merolla08082014}\end{minipage} &Digital& & & & & & &46 GSops/W & \\
       \begin{minipage}{1.8cm}\centering Neurogrid \citep{Benjamin_etal14}\end{minipage} &Mixed-mode& & & & & & & & \\
       \begin{minipage}{1.8cm}\centering HI-CANN \citep{Schemmel_etal10}  \end{minipage} &Mixed-mode & & & & & & & & \\
       \begin{minipage}{1.8cm}\centering HiAER-IFAT \citep{gert}\end{minipage} &Mixed-mode & & & & & & & & 
       
    \end{tabular}
    \egroup
  \end{center}
  \label{tb:hardware_comparison}
\end{table*}
    
%table summary?
