\section{Introduction}
\label{sec:intro}
%\subsection{What Is the Problem}
With the rapid development on neural engineering, researchers are approaching the aims of understanding the brain functions and building brain-like machines upon the knowledge~\cite{furber2007neural}. 
As a fast growing field, neuromorphic engineering has provided biologically-inspired sensors such as DVS(Dynamic Vision Sensor) silicon retinas~\cite{serrano-gotarredona_128_2013, lichtsteiner2008128}, which are good examples of low-cost visual processing thanks to their event-driven and redundancy-reducing style of computation.
Moreover, SNNs simulation tools~\cite{davison2008pynn, gewaltig2007nest, goodman2008brian} and neuromorphic hardware platforms~\cite{furber2013overview,  schemmel2010wafer, moradi2014event} have been developed to allow exploration of the brain by mimicking its functions and developing large-scale practical applications.
SNN has become an active area of computer vision thanks to the explicit  biological study on the central visual pathway.
The central visual system consists of several cortical areas responsible for visual processing, which are placed in a hierarchical pattern according to the anatomical experiments~\cite{felleman1991distributed}.
Fast object recognition proceeds in  the feed-forward hierarchy of the ventral pathway, and the information is unfolded along the stream to the  IT (Inferior Temporal) cortex~\cite{dicarlo2012does}.
%There are two basic streams locating in the visual area: a dorsal and a ventral pathway.
%They differ in behavioural patterns according to the observation from brain lesions~\cite{prado2005two}, and also in functions where the ventral (`perception') stream perceives the world by means of object recognition and memory, while the dorsal (`action') stream provides real-time visual guidance for motor actions such as eye movements and grasping objects~\cite{goodale1992separate}. 

Riesenhuber and Poggio (\cite{riesenhuber1999hierarchical}) proposed a quantitative modelling framework of object recognition with position-, scale- and view-invariance based on the units of MAX-like operations.
The cortical-like model has been analysed on several datasets~\cite{serre2007robust}.
And recently Fu and etc. (\cite{fu_spiking_2012}) reported their SNN implementation of the framework was capable of facial expression recognition tasks (97.35\% on JAFFE dataset~\cite{lyons1998coding} which contains 213 images of 7 facial expressions posed by 10 individuals).
% 97.35\% on JAFFE dataset.
They employed simple integrate-and-fire neurons with rank order coding (ROC) where  the earliest pre-synaptic spikes have the strongest impact on the post synaptic potentials.
According to Thorpe~\cite{vanrullen_surfing_2002}, the first wave of spikes  carry explicit information through the ventral stream and in each stage meaningful information is extracted and spikes are regenerated. 
Using one spike per neuron, Delorme and Thorpe~\cite{delorme_face_2001} reported 100\% and 97.5\% of accuracies on the face identification task over changing  contrast and luminance training (40 individuals $\times$ 8 images) and testing data  (40 individuals $\times$ 2 images) respectively.
%These developments yielded a large number of papers on SNNs based recognition, with a majority reporting outstanding recognition resulton limited-size databases.

Convolutional Neural Network (CNN), also known as \textit{ConvNet} which is carried out by Lecun and Bengio (\cite{lecun1995convolutional}), is a well applied model of such a cortical-like framework.
%Reported results:
%Hand Gestures, Qian Liu
An early Convolutional Spiking Neural Network (CSNN) model identified faces of 35 persons with an accuracy of 98.3\% exploiting simple integrate and fire neurons~\cite{matsugu2002convolutional}.
Another CSNN model~\cite{zhao_feedforward_2014} was trained and tested both with DVS raw data and Leaky Integrate-and-Fire (LIF) neurons.
%The MAX operation, training and the switch are not only neuron involved.
It was capable of recognising three moving postures with an accuracy about 99.48\% and 88.14\% on the MNIST-DVS dataset (see Chapter~\ref{sec:data}).
As one step forward, the paper (\cite{camunas2012event}) implemented a convolution processor module in hardware which could be combined with an DVS for high-speed recognition tasks.
The input of the ConvNet was continuous spike events instead of static images or frame-based videos. 
The chip detected four suits of a 52 card deck while the cards were fast browsed in only 410 ms.
Similarly, a real-time gesture recognition model~\cite{liu2014real} was implemented on a neuromorphic system with a DVS as a front-end and a SpiNNaker~\cite{furber2013overview} machine as the back-end where LIF neurons built up the ConvNet configured with biological parameters.
In this study's largest configuration, a network of 74,210 neurons and 15,216,512 synapses used 290 SpiNNaker cores in parallel and reached 93.0\% accuracy. 

Deep neural networks together with deep learning are the most exciting research fields in vision recognition.
Spiking deep network is of great potential to combine the remarkable performance and a more energy efficient training and running.
In the initial stage of the research, the study was focused on converting off-line trained deep network to SNNs~\cite{o2013real}.
The performance was increased from 92.0\% to 95.0\%~\cite{unpublished_EVAN} by implementing the model on SpiNNaker instead of the earlier FPGA version.
Recent attempts have been contributed to better translation by utilising modified units in ConvNet (\cite{cao2015spiking}) and tuning the weights and thresholds (\cite{unpublished_peter}).
They both accomplished state-of-the-art performance (\% and 99.1\% on MNIST dataset) comparing to original SNNs.
Current trend of training Spiking DNNs on-line using biologically-plausible learning methods is also promising.
An event driven Contrastive Divergence (CD) training algorithm for SNNs was proposed and verified on MNIST (91.9\%).


%Hand Gestures, Lee
The motion trajectories of moving hands are detected by spatiotemporally correlating the stereoscopically verged asynchronous events from the DVSs by using leaky integrate-and-fire (LIF) neurons. Adaptive thresholds of the LIF neurons achieve the segmentation of trajectories, which are then translated into discrete and finite feature vectors. The feature vectors are classified with hidden Markov models, using a separate Gaussian mixture model for spotting irrelevant transition gestures. The disparity information from stereovision is used to adapt LIF neuron parameters to achieve recognition invariant of the distance of the user to the sensor, and also helps to filter out movements in the background of the user. Exploiting the high dynamic range of DVSs, furthermore, allows gesture recognition over a 60-dB range of scene illuminance. The system achieves recognition rates well over 90\% under a variety of variable conditions with static and dynamic backgrounds with naiÌˆve users.
%MNIST unsupervised STDP unpublished
%Presented in Poissonian spike trains, the MNIST hand-written digits could be classified correctly with 95.0\% accuracy based on STDP unsupervised learning.
%The performance was achieved by 6,400 classifier neurons and presenting 15 times the entire MNIST training set.
%MNSIT Emre Neftci
%The recurrent activity of the network replaces the discrete steps of the CD algorithm, while Spike Time Dependent Plasticity (STDP) carries out the weight updates in an online, asynchronous fashion. We demonstrate our approach by training an RBM composed of leaky I&F neurons with STDP synapses to learn a generative model of the MNIST hand-written digit dataset, and by testing it in recognition, generation and cue integration tasks. Our results contribute to a machine learning-driven approach for synthesizing networks of spiking neurons capable of carrying out practical, high-level functionality.
%MNIST Evangelos Stromatias
In  this  context  we  in-
troduce  a  realization  of  a  spike-based  variation  of  previously
trained  DBNs  on  the  biologically-inspired  parallel  SpiNNaker
platform. The DBN on SpiNNaker runs in real-time and achieves
a classification performance of 95% on the MNIST handwritten
digit  dataset,  which  is  only  0.06%  less  than  that  of  a  pure
software implementation. Importantly, using a neurally-inspired
architecture yields additional benefits: during network run-time
on this task, the platform consumes only 0.3 W with classification
latencies in the order of tens of milliseconds, making it suitable
for  implementing  such  networks  on  a  mobile  platform.  The
results in this paper also show how the power dissipation of the
SpiNNaker platform and the classification latency of a network
scales with the number of neurons and layers in the network and
the overall spike activity rate.


Few of these algorithms reported results on images from a common database; fewer met the desirable goal of being evaluated against a standard testing protocol that includes separate training and testing sets.
As a consequence, there was no way to make a quantitative assessment of the algorithms' relative strengths and weaknesses.
Unfortunately, this is not an isolated case, but an endemic problem in computer vision research.



\subsection{The Proposal: Advantages}
In this paper we present a comprehensive method of eval-
uating face-recognition
algorithms, developed as part of the
Face Recognition Technology (FERET) program [ 10,111.
The FERET evaluation methodology consists of an inte-
grated data collection effort and testing program. These
two parts are integrated through the FERET database of
facial images; the database is divided into a development
portion, which is provided to researchers, and a sequestered
portion for testing. The partition of the database enables
algorithms to be trained and tested on different, but related,
sets of images. The FERET evaluation procedure is a set of
standard testing protocols: the FERET tests are indepen-
dently administered
and each test is completed within
three days. The use of a standard testing protocol allows
for the direct comparison of algorithms developed by dif-
ferent groups, as well as for measuring improvements made
by any single group over time.

First of all, an initial dataset of input stimuli based on standard computer vision benchmarks consisting of %facial images (FERET database) and 
digits (MNIST database) is presented according to the current research on spike-based image recognition.
Within this dataset, all images are centre aligned and with similar scale.
We describe how we intend to expand this dataset to fulfil the needs of upcoming research problems.
For instance, the data should provide cases to measure position-, scale-, and viewing-angle invariance.
The data will be in Address-Event Representation (AER) format which is well-applied in neuromorphic engineering field unlike conventional images.
These spike trains are produced by various techniques: rate-based Poisson spike generation, rank order encoding and recorded output from a silicon retina with both flashing and oscillating input stimuli.
An evaluation methodology is also presented which describes how to consistently assess the accuracy, speed, efficiency and cost of an algorithm working with the dataset.
Finally, we provide a baseline for comparison based on a proposed SNN's performance on the dataset.
The network is trained on-line using the Spike Timing Dependent Plasticity (STDP) learning rule on a massive-parallel neuromorphic simulator, e.g. SpiNNaker.